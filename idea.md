Here's a comprehensive README.md that will guide an AI agent to build the complete system:

```markdown
# PromptGuard ğŸ›¡ï¸

**The Production-Ready Framework for Reliable LLM Orchestration**

PromptGuard is a Python library that brings production-grade reliability, type safety, and observability to LLM applications. Think of it as "Pydantic meets Circuit Breaker for AI" - reducing boilerplate by 80% while making your AI apps bulletproof.

[![PyPI version](https://badge.fury.io/py/promptguard.svg)](https://badge.fury.io/py/promptguard)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![Python 3.9+](https://img.shields.io/badge/python-3.9+-blue.svg)](https://www.python.org/downloads/)

---

## ğŸ¯ The Problem We Solve

Every AI engineer writes the same boilerplate:
- Manual retry logic with exponential backoff
- Model fallback chains when primary fails
- Response parsing with regex/string manipulation
- Token counting and cost tracking
- Response validation and error handling

**PromptGuard eliminates all of this.**

---

## âœ¨ Core Features

### 1. Smart Execution with Auto-Retry & Fallbacks
```python
from promptguard import PromptChain

chain = PromptChain(
    models=["claude-3-5-sonnet-20241022", "gpt-4o", "groq/llama-70b"],
    strategy="cascade",  # Try models in order until success
    max_retries=3,
    retry_delay="exponential"  # 1s, 2s, 4s, 8s...
)

result = await chain.execute("Analyze this document...")
print(result.response)  # Guaranteed to succeed or raise clear error
```

### 2. Type-Safe Response Validation (Pydantic Integration)
```python
from pydantic import BaseModel, Field
from promptguard import PromptChain

class EvaluationResponse(BaseModel):
    evaluation: str = Field(description="Overall evaluation")
    score: int = Field(ge=0, le=100, description="Score 0-100")
    reason: str = Field(min_length=20, description="Detailed reasoning")
    references: list[str] = Field(description="Source citations")

chain = PromptChain(
    models=["claude-3-5-sonnet-20241022"],
    response_schema=EvaluationResponse,
    validation_mode="strict"  # Force retry if schema validation fails
)

result = await chain.execute(prompt)
#result.response is typed as EvaluationResponse
print(result.response.score)  # Type-safe access
```

### 3. Semantic Response Validation
```python
from promptguard import PromptChain, validators

chain = PromptChain(
    models=["claude-3-5-sonnet-20241022"],
    validators=[
        validators.no_hallucination(source_docs=documents),
        validators.has_citations(required=True),
        validators.length_range(min_chars=100, max_chars=5000),
        validators.contains_keywords(["risk", "evaluation"]),
        validators.sentiment_check(allowed=["neutral", "positive"])
    ]
)

result = await chain.execute(prompt)
# All validators passed, or auto-retry triggered
```

### 4. Multi-Provider Support (Provider-Agnostic)
```python
chain = PromptChain(
    models=[
        "anthropic/claude-3-5-sonnet-20241022",
        "openai/gpt-4o",
        "groq/llama-3-70b-8192",
        "cohere/command-r-plus",
        "google/gemini-1.5-pro"
    ],
    strategy="fastest"  # Race models, use fastest response
)
```

### 5. Automatic Token Tracking & Cost Estimation
```python
result = await chain.execute(prompt)

print(result.metadata.tokens_used)       # {input: 1234, output: 567}
print(result.metadata.estimated_cost)    # $0.0234
print(result.metadata.model_used)        # "claude-3-5-sonnet-20241022"
print(result.metadata.attempts)          # 1
print(result.metadata.execution_time_ms) # 1234
```

### 6. Response Caching (Redis/In-Memory)
```python
from promptguard import PromptChain, CacheBackend

chain = PromptChain(
    models=["claude-3-5-sonnet-20241022"],
    cache=CacheBackend.redis(url="redis://localhost:6379"),
    cache_ttl=3600  # 1 hour
)

# First call hits API
result1 = await chain.execute("What is AI?", cache_key="ai_definition")

# Second call returns cached result (instant)
result2 = await chain.execute("What is AI?", cache_key="ai_definition")
assert result1.metadata.cached == False
assert result2.metadata.cached == True
```

### 7. Streaming Support
```python
chain = PromptChain(models=["claude-3-5-sonnet-20241022"])

async for chunk in chain.stream("Write a long essay..."):
    print(chunk.delta, end="", flush=True)
    # Also access: chunk.metadata, chunk.accumulated_text
```

### 8. Batch Processing with Concurrency Control
```python
prompts = [
    "Evaluate document 1...",
    "Evaluate document 2...",
    # ... 100 more
]

results = await chain.batch_execute(
    prompts,
    max_concurrent=5,  # Process 5 at a time
    show_progress=True  # Display progress bar
)

for result in results:
    if result.success:
        print(result.response)
    else:
        print(f"Failed: {result.error}")
```

### 9. Custom Retry Strategies
```python
from promptguard import RetryStrategy

def custom_retry_logic(attempt: int, error: Exception) -> float:
    """Return delay in seconds, or raise to stop retrying"""
    if "rate_limit" in str(error):
        return 60  # Wait 1 minute for rate limits
    elif attempt < 3:
        return 2 ** attempt  # Exponential backoff
    else:
        raise error  # Give up after 3 attempts

chain = PromptChain(
    models=["claude-3-5-sonnet-20241022"],
    retry_strategy=RetryStrategy.custom(custom_retry_logic)
)
```

### 10. Observability & Logging
```python
from promptguard import PromptChain, LogLevel

chain = PromptChain(
    models=["claude-3-5-sonnet-20241022"],
    log_level=LogLevel.DEBUG,
    metrics_backend="prometheus",  # or "datadog", "custom"
    on_success=lambda result: print(f"Success: {result.metadata}"),
    on_retry=lambda attempt, error: print(f"ğŸ”„ Retry {attempt}: {error}"),
    on_failure=lambda error: print(f"Failed: {error}")
)
```

---

## ğŸ—ï¸ Architecture & Design Principles

### Core Components

```
promptguard/
â”œâ”€â”€ core/
â”‚   â”œâ”€â”€ chain.py              # Main PromptChain orchestrator
â”‚   â”œâ”€â”€ executor.py           # Execution engine with retry logic
â”‚   â”œâ”€â”€ response.py           # Response models and metadata
â”‚   â””â”€â”€ models.py             # Model registry and configuration
â”œâ”€â”€ providers/
â”‚   â”œâ”€â”€ base.py               # Base provider interface
â”‚   â”œâ”€â”€ anthropic_provider.py # Claude integration
â”‚   â”œâ”€â”€ openai_provider.py    # OpenAI integration
â”‚   â”œâ”€â”€ groq_provider.py      # Groq integration
â”‚   â”œâ”€â”€ cohere_provider.py    # Cohere integration
â”‚   â””â”€â”€ google_provider.py    # Google integration
â”œâ”€â”€ validation/
â”‚   â”œâ”€â”€ schema.py             # Pydantic schema validation
â”‚   â”œâ”€â”€ semantic.py           # Semantic validators
â”‚   â””â”€â”€ custom.py             # Custom validator framework
â”œâ”€â”€ caching/
â”‚   â”œâ”€â”€ base.py               # Cache interface
â”‚   â”œâ”€â”€ memory.py             # In-memory cache
â”‚   â”œâ”€â”€ redis.py              # Redis cache backend
â”‚   â””â”€â”€ disk.py               # Disk-based cache
â”œâ”€â”€ retry/
â”‚   â”œâ”€â”€ strategies.py         # Retry strategy implementations
â”‚   â””â”€â”€ backoff.py            # Backoff algorithms
â”œâ”€â”€ observability/
â”‚   â”œâ”€â”€ logger.py             # Structured logging
â”‚   â”œâ”€â”€ metrics.py            # Metrics collection
â”‚   â””â”€â”€ tracing.py            # Distributed tracing support
â”œâ”€â”€ utils/
â”‚   â”œâ”€â”€ token_counter.py      # Token counting utilities
â”‚   â”œâ”€â”€ cost_estimator.py     # Cost calculation
â”‚   â””â”€â”€ parser.py             # Response parsing helpers
â””â”€â”€ exceptions.py             # Custom exception hierarchy
```

### Design Principles

1. **Provider Agnostic**: Abstract away provider differences
2. **Type Safe**: Leverage Pydantic for runtime validation
3. **Async First**: Built on asyncio for performance
4. **Composable**: Mix and match features as needed
5. **Observable**: Built-in logging, metrics, and tracing
6. **Extensible**: Plugin system for custom validators, caches, providers
7. **Zero Config**: Sensible defaults, configure only when needed

---

## ğŸ“‹ Technical Specifications

### 1. PromptChain Class

**File: `promptguard/core/chain.py`**

```python
class PromptChain:
    """Main orchestrator for LLM prompt execution with reliability features."""
    
    def __init__(
        self,
        models: list[str],
        strategy: Literal["cascade", "fastest", "cheapest", "parallel"] = "cascade",
        max_retries: int = 3,
        retry_delay: Union[float, Literal["exponential", "fibonacci"]] = "exponential",
        timeout: float = 30.0,
        response_schema: Optional[Type[BaseModel]] = None,
        validation_mode: Literal["strict", "lenient"] = "strict",
        validators: Optional[list[Validator]] = None,
        cache: Optional[CacheBackend] = None,
        cache_ttl: int = 3600,
        log_level: LogLevel = LogLevel.INFO,
        metrics_backend: Optional[str] = None,
        on_success: Optional[Callable] = None,
        on_retry: Optional[Callable] = None,
        on_failure: Optional[Callable] = None,
        **provider_kwargs
    ):
        """
        Initialize PromptChain.
        
        Args:
            models: List of model identifiers (e.g., "anthropic/claude-3-5-sonnet")
            strategy: Execution strategy
                - cascade: Try models in order until success
                - fastest: Race all models, return fastest
                - cheapest: Try cheapest model first
                - parallel: Execute on all models and compare
            max_retries: Maximum retry attempts per model
            retry_delay: Delay between retries (seconds or strategy name)
            timeout: Request timeout in seconds
            response_schema: Pydantic model for response validation
            validation_mode: How to handle validation failures
                - strict: Retry on validation failure
                - lenient: Return invalid response with warning
            validators: List of semantic validators
            cache: Cache backend instance
            cache_ttl: Cache TTL in seconds
            log_level: Logging verbosity
            metrics_backend: Metrics collection backend
            on_success: Callback on successful execution
            on_retry: Callback on retry attempt
            on_failure: Callback on final failure
            **provider_kwargs: Provider-specific configuration
        """
    
    async def execute(
        self,
        prompt: Union[str, list[dict]],
        context: Optional[dict] = None,
        temperature: float = 0.01,
        max_tokens: int = 4096,
        cache_key: Optional[str] = None,
        metadata: Optional[dict] = None,
        **kwargs
    ) -> PromptResult:
        """
        Execute prompt with automatic retry and fallback.
        
        Args:
            prompt: Prompt string or message list
            context: Additional context for prompt formatting
            temperature: Model temperature
            max_tokens: Maximum output tokens
            cache_key: Cache key (if caching enabled)
            metadata: Custom metadata to attach to result
            **kwargs: Provider-specific arguments
            
        Returns:
            PromptResult with response, metadata, and execution info
            
        Raises:
            PromptExecutionError: If all retries/fallbacks fail
            ValidationError: If validation fails in strict mode
        """
    
    async def stream(
        self,
        prompt: Union[str, list[dict]],
        **kwargs
    ) -> AsyncIterator[StreamChunk]:
        """Stream response chunks with automatic retry."""
    
    async def batch_execute(
        self,
        prompts: list[Union[str, list[dict]]],
        max_concurrent: int = 5,
        show_progress: bool = False,
        fail_fast: bool = False,
        **kwargs
    ) -> list[PromptResult]:
        """Execute multiple prompts with concurrency control."""
```

### 2. Response Models

**File: `promptguard/core/response.py`**

```python
@dataclass
class ExecutionMetadata:
    """Metadata about prompt execution."""
    model_used: str
    provider: str
    attempts: int
    execution_time_ms: float
    tokens_used: dict[str, int]  # {input: X, output: Y, total: Z}
    estimated_cost: float
    cached: bool
    cache_key: Optional[str]
    retry_history: list[dict]  # [{attempt: 1, error: "...", delay: 2.0}]
    timestamp: datetime

class PromptResult:
    """Result of prompt execution."""
    success: bool
    response: Union[str, BaseModel, None]
    raw_response: Any  # Provider-specific response object
    metadata: ExecutionMetadata
    error: Optional[Exception]
    validation_results: Optional[dict]  # Results from semantic validators
    
    def to_dict(self) -> dict:
        """Convert to dictionary."""
    
    def to_json(self) -> str:
        """Convert to JSON string."""

@dataclass
class StreamChunk:
    """Single chunk from streaming response."""
    delta: str
    accumulated_text: str
    metadata: Optional[dict]
    finished: bool
```

### 3. Provider Interface

**File: `promptguard/providers/base.py`**

```python
class BaseProvider(ABC):
    """Base class for LLM providers."""
    
    @abstractmethod
    async def execute(
        self,
        messages: list[dict],
        model: str,
        temperature: float,
        max_tokens: int,
        **kwargs
    ) -> dict:
        """Execute prompt and return standardized response."""
    
    @abstractmethod
    async def stream(
        self,
        messages: list[dict],
        model: str,
        **kwargs
    ) -> AsyncIterator[str]:
        """Stream response chunks."""
    
    @abstractmethod
    def count_tokens(self, text: str, model: str) -> int:
        """Count tokens for given text and model."""
    
    @abstractmethod
    def estimate_cost(self, input_tokens: int, output_tokens: int, model: str) -> float:
        """Estimate cost based on token usage."""
    
    @abstractmethod
    def parse_response(self, raw_response: Any) -> dict:
        """Parse provider-specific response to standard format."""
    
    @property
    @abstractmethod
    def supported_models(self) -> list[str]:
        """List of supported model identifiers."""
```

### 4. Validation System

**File: `promptguard/validation/semantic.py`**

```python
class Validator(ABC):
    """Base class for response validators."""
    
    @abstractmethod
    async def validate(
        self,
        response: str,
        context: Optional[dict] = None
    ) -> ValidationResult:
        """Validate response and return result."""

@dataclass
class ValidationResult:
    passed: bool
    confidence: float  # 0.0 to 1.0
    message: str
    details: Optional[dict]

class NoHallucinationValidator(Validator):
    """Detect hallucinations by checking against source documents."""
    
    def __init__(self, source_docs: list[str], threshold: float = 0.8):
        self.source_docs = source_docs
        self.threshold = threshold
    
    async def validate(self, response: str, context: Optional[dict] = None) -> ValidationResult:
        # Use embedding similarity or NLI model to check claims
        pass

class CitationValidator(Validator):
    """Ensure response has proper citations."""
    
    def __init__(self, required: bool = True, format: str = "any"):
        pass

class LengthValidator(Validator):
    """Validate response length."""
    pass

class KeywordValidator(Validator):
    """Ensure response contains required keywords."""
    pass

class SentimentValidator(Validator):
    """Check response sentiment."""
    pass

# Convenience functions
def no_hallucination(source_docs: list[str], threshold: float = 0.8) -> NoHallucinationValidator:
    return NoHallucinationValidator(source_docs, threshold)

def has_citations(required: bool = True, format: str = "any") -> CitationValidator:
    return CitationValidator(required, format)

# ... more validators
```

### 5. Caching System

**File: `promptguard/caching/base.py`**

```python
class CacheBackend(ABC):
    """Base class for cache backends."""
    
    @abstractmethod
    async def get(self, key: str) -> Optional[dict]:
        """Retrieve cached result."""
    
    @abstractmethod
    async def set(self, key: str, value: dict, ttl: int) -> None:
        """Store result in cache."""
    
    @abstractmethod
    async def delete(self, key: str) -> None:
        """Remove from cache."""
    
    @abstractmethod
    async def clear(self) -> None:
        """Clear all cache entries."""
    
    @staticmethod
    def memory() -> "MemoryCacheBackend":
        """Create in-memory cache."""
        from .memory import MemoryCacheBackend
        return MemoryCacheBackend()
    
    @staticmethod
    def redis(url: str = "redis://localhost:6379") -> "RedisCacheBackend":
        """Create Redis cache backend."""
        from .redis import RedisCacheBackend
        return RedisCacheBackend(url)
    
    @staticmethod
    def disk(path: str = ".promptguard_cache") -> "DiskCacheBackend":
        """Create disk-based cache."""
        from .disk import DiskCacheBackend
        return DiskCacheBackend(path)
```

### 6. Retry Strategies

**File: `promptguard/retry/strategies.py`**

```python
class RetryStrategy(ABC):
    """Base class for retry strategies."""
    
    @abstractmethod
    def get_delay(self, attempt: int, error: Exception) -> float:
        """Calculate delay before next retry."""
    
    @abstractmethod
    def should_retry(self, attempt: int, error: Exception) -> bool:
        """Determine if should retry."""

class ExponentialBackoff(RetryStrategy):
    """Exponential backoff: 1s, 2s, 4s, 8s..."""
    
    def __init__(self, base: float = 1.0, max_delay: float = 60.0):
        self.base = base
        self.max_delay = max_delay
    
    def get_delay(self, attempt: int, error: Exception) -> float:
        return min(self.base * (2 ** attempt), self.max_delay)

class FibonacciBackoff(RetryStrategy):
    """Fibonacci backoff: 1s, 1s, 2s, 3s, 5s, 8s..."""
    pass

class ConstantDelay(RetryStrategy):
    """Fixed delay between retries."""
    pass

class CustomRetryStrategy(RetryStrategy):
    """User-defined retry logic."""
    
    def __init__(self, func: Callable[[int, Exception], float]):
        self.func = func
```

### 7. Exception Hierarchy

**File: `promptguard/exceptions.py`**

```python
class PromptGuardError(Exception):
    """Base exception for PromptGuard."""
    pass

class PromptExecutionError(PromptGuardError):
    """Error during prompt execution."""
    
    def __init__(self, message: str, attempts: list[dict], last_error: Exception):
        self.attempts = attempts
        self.last_error = last_error
        super().__init__(message)

class ValidationError(PromptGuardError):
    """Response validation failed."""
    
    def __init__(self, message: str, validation_results: dict):
        self.validation_results = validation_results
        super().__init__(message)

class ModelNotFoundError(PromptGuardError):
    """Requested model not available."""
    pass

class RateLimitError(PromptGuardError):
    """Rate limit exceeded."""
    
    def __init__(self, retry_after: Optional[float] = None):
        self.retry_after = retry_after
        super().__init__(f"Rate limit exceeded. Retry after {retry_after}s")

class TimeoutError(PromptGuardError):
    """Request timed out."""
    pass
```

---

## ğŸ”§ Implementation Roadmap

### Phase 1: Core Foundation (Week 1-2)
- [ ] Project setup (poetry, pre-commit, CI/CD)
- [ ] Basic PromptChain class with single model support
- [ ] Anthropic provider implementation
- [ ] Simple retry logic with exponential backoff
- [ ] Basic response models and metadata
- [ ] Unit tests for core functionality

### Phase 2: Multi-Provider Support (Week 3-4)
- [ ] Provider abstraction layer
- [ ] OpenAI provider
- [ ] Groq provider
- [ ] Model fallback/cascade strategy
- [ ] Token counting utilities
- [ ] Cost estimation

### Phase 3: Validation System (Week 5-6)
- [ ] Pydantic schema validation
- [ ] Semantic validator framework
- [ ] No-hallucination validator (using embeddings)
- [ ] Citation validator
- [ ] Length/keyword/sentiment validators
- [ ] Validation result reporting

### Phase 4: Advanced Features (Week 7-8)
- [ ] Caching system (memory, Redis, disk)
- [ ] Streaming support
- [ ] Batch processing with concurrency control
- [ ] Fastest/cheapest/parallel strategies
- [ ] Custom retry strategies

### Phase 5: Observability (Week 9-10)
- [ ] Structured logging
- [ ] Metrics collection (Prometheus, DataDog)
- [ ] Distributed tracing support
- [ ] Performance monitoring
- [ ] Debug mode with detailed logs

### Phase 6: Polish & Documentation (Week 11-12)
- [ ] Comprehensive documentation
- [ ] Example notebooks
- [ ] Integration guides
- [ ] Performance benchmarks
- [ ] Security audit
- [ ] PyPI publishing

---

## ğŸ“¦ Dependencies

**`pyproject.toml`**:
```toml
[tool.poetry.dependencies]
python = "^3.9"
pydantic = "^2.5.0"
anthropic = "^0.21.0"
openai = "^1.12.0"
aiohttp = "^3.9.0"
redis = {version = "^5.0.0", optional = true}
tiktoken = "^0.6.0"  # Token counting
sentence-transformers = {version = "^2.3.0", optional = true}  # For hallucination detection
prometheus-client = {version = "^0.19.0", optional = true}
structlog = "^24.1.0"

[tool.poetry.extras]
cache = ["redis"]
validation = ["sentence-transformers"]
metrics = ["prometheus-client"]
all = ["redis", "sentence-transformers", "prometheus-client"]

[tool.poetry.group.dev.dependencies]
pytest = "^7.4.0"
pytest-asyncio = "^0.23.0"
pytest-cov = "^4.1.0"
black = "^23.12.0"
ruff = "^0.1.0"
mypy = "^1.8.0"
pre-commit = "^3.6.0"
```

---

## ğŸ§ª Testing Strategy

### Test Structure
```
tests/
â”œâ”€â”€ unit/
â”‚   â”œâ”€â”€ test_chain.py
â”‚   â”œâ”€â”€ test_providers.py
â”‚   â”œâ”€â”€ test_validators.py
â”‚   â”œâ”€â”€ test_caching.py
â”‚   â””â”€â”€ test_retry.py
â”œâ”€â”€ integration/
â”‚   â”œâ”€â”€ test_anthropic_integration.py
â”‚   â”œâ”€â”€ test_openai_integration.py
â”‚   â””â”€â”€ test_end_to_end.py
â”œâ”€â”€ fixtures/
â”‚   â””â”€â”€ sample_responses.json
â””â”€â”€ conftest.py
```

### Test Coverage Requirements
- Unit tests: 90%+ coverage
- Integration tests for each provider
- End-to-end tests for common workflows
- Performance benchmarks
- Stress tests (rate limits, timeouts, failures)

### Example Test
```python
# tests/unit/test_chain.py
import pytest
from promptguard import PromptChain
from pydantic import BaseModel

class TestResponse(BaseModel):
    answer: str
    confidence: float

@pytest.mark.asyncio
async def test_basic_execution():
    chain = PromptChain(models=["anthropic/claude-3-5-sonnet"])
    result = await chain.execute("What is 2+2?")
    assert result.success
    assert "4" in result.response

@pytest.mark.asyncio
async def test_schema_validation():
    chain = PromptChain(
        models=["anthropic/claude-3-5-sonnet"],
        response_schema=TestResponse
    )
    result = await chain.execute("What is AI?")
    assert isinstance(result.response, TestResponse)

@pytest.mark.asyncio
async def test_fallback():
    chain = PromptChain(
        models=["fake/model", "anthropic/claude-3-5-sonnet"],
        strategy="cascade"
    )
    result = await chain.execute("Test")
    assert result.success
    assert result.metadata.attempts == 2
```

---

## ğŸ“š Documentation Requirements

### 1. Getting Started Guide
- Installation instructions
- Quick start examples
- Basic configuration

### 2. API Reference
- Complete class/method documentation
- Parameter descriptions
- Return type specifications
- Usage examples for each feature

### 3. Cookbook / Examples
- Common patterns
- Integration examples (FastAPI, LangChain, etc.)
- Production best practices
- Performance optimization

### 4. Migration Guides
- From raw API calls
- From LangChain
- From other libraries

### 5. Advanced Topics
- Custom providers
- Custom validators
- Performance tuning
- Security considerations

---

## ğŸš€ Quick Start Example

```python
# install: pip install promptguard[all]

from promptguard import PromptChain, validators, CacheBackend
from pydantic import BaseModel

# Define response schema
class Analysis(BaseModel):
    summary: str
    score: int
    recommendations: list[str]

# Create chain with all features
chain = PromptChain(
    models=[
        "anthropic/claude-3-5-sonnet-20241022",
        "openai/gpt-4o",
        "groq/llama-70b"
    ],
    strategy="cascade",
    max_retries=3,
    response_schema=Analysis,
    validators=[
        validators.length_range(min_chars=100),
        validators.has_citations(required=True)
    ],
    cache=CacheBackend.redis(),
    cache_ttl=3600
)

# Execute with automatic retry, fallback, and validation
result = await chain.execute(
    prompt="Analyze this document: ...",
    cache_key="doc_analysis_v1"
)

# Access typed response
print(result.response.score)  # Type-safe!
print(result.metadata.estimated_cost)  # $0.023
print(result.metadata.model_used)  # "claude-3-5-sonnet"
```

---

## ğŸ¯ Success Metrics

**Adoption Metrics:**
- 1,000+ GitHub stars in 6 months
- 10,000+ downloads/month on PyPI
- Used in production by 50+ companies

**Technical Metrics:**
- <10ms overhead per request
- 99.9%+ reliability with fallbacks
- 80%+ reduction in boilerplate code

**Community Metrics:**
- 10+ external contributors
- Integration with major frameworks (LangChain, LlamaIndex)
- Featured in AI engineering blogs/podcasts

---

## ğŸ¤ Contributing Guidelines

1. Fork repository
2. Create feature branch
3. Write tests (coverage >90%)
4. Update documentation
5. Submit PR with clear description

**Code Style:**
- Black for formatting
- Ruff for linting
- Type hints required
- Docstrings for all public APIs

---

## ğŸ“„ License

MIT License - See LICENSE file for details

---

## ğŸ™ Acknowledgments

Inspired by production challenges in real-world AI applications, particularly in the oil & gas industry where reliability is critical.

---

## ğŸ“ Support

- ğŸ“– Documentation: https://promptguard.dev
- ğŸ’¬ Discord: https://discord.gg/promptguard
- ğŸ› Issues: https://github.com/promptguard/promptguard/issues
- ğŸ“§ Email: support@promptguard.dev

---

**Built with â¤ï¸ for the AI engineering community**
```

---

## Additional Files to Create

### 1. `ARCHITECTURE.md` - Detailed Architecture Document
```markdown
# PromptGuard Architecture

## System Design

[Detailed architecture diagrams, data flow, component interactions]

## Design Decisions

[Why certain choices were made, trade-offs considered]

## Extension Points

[How to add custom providers, validators, etc.]
```

### 2. `CONTRIBUTING.md` - Contribution Guidelines
```markdown
# Contributing to PromptGuard

[Detailed contribution process, code review criteria, release process]
```

### 3. `examples/` Directory
Create examples for:
- Basic usage
- Advanced patterns
- Production deployment
- Integration with frameworks